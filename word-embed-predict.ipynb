{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "CONTEXT_SIZE = 4 ### the number of size that we need to input for prediction \n",
    "EMBEDDING_DIM = 10 ### how many dimensions of matrix to represent a word\n",
    "\n",
    "test_sentence = \"\"\"When forty winters shall besiege thy brow,\n",
    "And dig deep trenches in thy beauty's field,\n",
    "Thy youth's proud livery so gazed on now,\n",
    "Will be a totter'd weed of small worth held:\n",
    "Then being asked, where all thy beauty lies,\n",
    "Where all the treasure of thy lusty days;\n",
    "To say, within thine own deep sunken eyes,\n",
    "Were an all-eating shame, and thriftless praise.\n",
    "How much more praise deserv'd thy beauty's use,\n",
    "If thou couldst answer 'This fair child of mine\n",
    "Shall sum my count, and make my old excuse,'\n",
    "Proving his beauty by succession thine!\n",
    "This were to be new made when thou art old,\n",
    "And see thy blood warm when thou feel'st it cold.\"\"\".split()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use previous 4 words as input to predict the fifth word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step1ï¼šCreate the four-grams with target word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gram = [((test_sentence[i], test_sentence[i+1],test_sentence[i+2], test_sentence[i+3]), test_sentence[i+4]) \n",
    "            for i in range(len(test_sentence)-CONTEXT_SIZE)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## encode words to be ints\n",
    "vocb = set(test_sentence)\n",
    "word_to_idx = {word: i for i, word in enumerate(vocb)}\n",
    "idx_to_word = {word_to_idx[word]: word for word in word_to_idx}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for word, label in gram[:100]:\n",
    "    word = [word_to_idx[i] for i in word] ### data sctructure :  word: ('When', 'forty','winters', 'shall') to extract the index\n",
    "    label = [word_to_idx[label]] ## extract the index with corresponding lables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[74, 62, 89, 59]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.contrib.slim as slim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "97"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word_to_idx)  ## the totoal number of different word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### inputs  = [50, 55, 58, 3], vocab_size: how many word that I need to create a matrix\n",
    "### n_dim : how many dim that you need to vectorize a word = 10 \n",
    "### vocab_size: how totoal word that I need to genearte a matrix\n",
    "\n",
    "def n_gram(inputs, vocab_size, context_size=CONTEXT_SIZE, n_dim=EMBEDDING_DIM, scope='n-gram', reuse=tf.AUTO_REUSE):\n",
    "    with tf.variable_scope(scope, reuse=reuse):\n",
    "        with tf.device('/cpu:0'):\n",
    "            \n",
    "            ### create a embeddings matrix with 97words with 10 dimension\n",
    "            embeddings = tf.get_variable('embeddings', shape=[vocab_size, n_dim], initializer=tf.random_uniform_initializer)\n",
    "        # size of embedding = [97,10]\n",
    "        \n",
    "\n",
    "        # to extract the two vectors that contain input(two word) information\n",
    "        embed = tf.nn.embedding_lookup(embeddings, inputs)\n",
    "        \n",
    "        \n",
    "        # combine four vectors to be one vector\n",
    "        net = tf.reshape(embed, (1, -1))\n",
    "        \n",
    "        \n",
    "        # fully connection to (vocab_size = 97) a vector that has 97 dims to present four words\n",
    "        net = slim.fully_connected(net, vocab_size, activation_fn=None, scope='classification')\n",
    "        \n",
    "        return net,embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_ph = tf.placeholder(dtype=tf.int64, shape=[4 ], name='input') ## [53,43,54,21]\n",
    "label_ph = tf.placeholder(dtype=tf.int64, shape=[1,], name='label')#[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "net,embeddings = n_gram(input_ph,len(word_to_idx)) ### forword-prediction completed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### define the loss fuction\n",
    "loss = tf.losses.sparse_softmax_cross_entropy(label_ph, net, scope='loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "opt = tf.train.MomentumOptimizer(1e-2, 0.9)\n",
    "train_op = opt.minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py:1702: UserWarning: An interactive session is already active. This can cause out-of-memory errors in some cases. You must explicitly call `InteractiveSession.close()` to release resources held by the other session(s).\n",
      "  warnings.warn('An interactive session is already active. This can '\n"
     ]
    }
   ],
   "source": [
    "sess = tf.InteractiveSession()\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 20, Loss: 0.032060\n",
      "Epoch: 40, Loss: 0.012355\n",
      "Epoch: 60, Loss: 0.007572\n",
      "Epoch: 80, Loss: 0.005407\n",
      "Epoch: 100, Loss: 0.004180\n",
      "Epoch: 120, Loss: 0.003393\n",
      "Epoch: 140, Loss: 0.002848\n",
      "Epoch: 160, Loss: 0.002448\n",
      "Epoch: 180, Loss: 0.002143\n",
      "Epoch: 200, Loss: 0.001904\n"
     ]
    }
   ],
   "source": [
    "#### train 200 times\n",
    "for e in range(200):\n",
    "    train_loss = 0\n",
    "    for word, label in gram:\n",
    "        word = [word_to_idx[i] for i in word] ### data sctructure :  word: ('When', 'forty') to extract the index\n",
    "        label = [word_to_idx[label]] ## extract the index with corresponding lables\n",
    "        \n",
    "        ## word : [65,32,43,21], lables = [5]\n",
    "        _, curr_loss = sess.run([train_op, loss], feed_dict={input_ph: word, label_ph: label})\n",
    "        train_loss += curr_loss\n",
    "    \n",
    "    if (e + 1) % 20 == 0:\n",
    "        print('Epoch: {}, Loss: {:.6f}'.format(e + 1, train_loss / 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input: ('so', 'gazed', 'on', 'now,')\n",
      "label: Will\n",
      "\n",
      "real word is Will, predicted word is Will\n"
     ]
    }
   ],
   "source": [
    "## test the result\n",
    "word, label = gram[19]\n",
    "print('input: {}'.format(word))\n",
    "print('label: {}'.format(label))\n",
    "print()\n",
    "word = [word_to_idx[i] for i in word]\n",
    "out = sess.run(net, feed_dict={input_ph: word})\n",
    "pred_label_idx = out[0].argmax()\n",
    "predict_word = idx_to_word[pred_label_idx]\n",
    "print('real word is {}, predicted word is {}'.format(label, predict_word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input: (\"'This\", 'fair', 'child', 'of')\n",
      "label: mine\n",
      "\n",
      "real word is mine, predicted word is mine\n"
     ]
    }
   ],
   "source": [
    "word, label = gram[75]\n",
    "print('input: {}'.format(word))\n",
    "print('label: {}'.format(label))\n",
    "print()\n",
    "word = [word_to_idx[i] for i in word]\n",
    "out = sess.run(net, feed_dict={input_ph: word})\n",
    "pred_label_idx = out[0].argmax()\n",
    "predict_word = idx_to_word[pred_label_idx]\n",
    "print('real word is {}, predicted word is {}'.format(label, predict_word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### to see a trained word-embed\n",
    "embeddings_matrix = embeddings.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(97, 10)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.04376271,  1.1123128 ,  0.4999244 , -0.84200466, -0.8110467 ,\n",
       "         0.2883291 ,  0.4943249 , -0.80180794, -0.7253908 , -0.24113272],\n",
       "       [ 1.5748116 , -0.10264023,  1.508522  , -0.88337845,  0.21163328,\n",
       "         0.10263743, -0.55512154, -0.05898177, -0.05966415,  1.1262923 ],\n",
       "       [ 1.1398656 ,  1.0337118 ,  0.27517706,  0.8770043 ,  0.64252543,\n",
       "         0.93555075, -0.5430839 ,  0.00470811, -0.5112552 ,  1.2213902 ]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings_matrix[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
